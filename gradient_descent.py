# -*- coding: utf-8 -*-
"""Gradient Descent.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Ct1BXZfVp0cX6lMlXKp0ujMr7qxCWy0Q

##Introduction
This notebook explores the Car Prices Prediction Dataset to build a predictive model for estimating car prices based on various features. The dataset consists of information about different car models, including the make, model, year of manufacture, mileage, and condition, among other attributes. The goal of this analysis is to understand how these factors influence the price of a car and to develop a linear regression model that can accurately predict car prices.

###Key features of the dataset include:

* Make: The manufacturer of the car (e.g., Ford, Toyota, Chevrolet).
* Model: The specific model of the car (e.g., Silverado, Civic).
* Year: The year in which the car was manufactured.
* Mileage: The total distance traveled by the car.
* Condition: The overall condition of the car (e.g., Excellent, Good).
* Price: The target variable representing the car's price.

###Objective
The primary objective of this project is to build a linear regression model using Gradient Descent to predict car prices based on the available features. In this notebook, the following steps will be undertaken:

####1) Data Preprocessing: Cleaning the data, handling categorical variables, and normalizing the feature set.
####2) Exploratory Data Analysis (EDA): Understanding relationships between features and the target variable (Price).
####3) Modeling: Implementing a linear regression model using Gradient Descent.
####4) Evaluation: Assessing the model's performance using evaluation metrics such as Mean Squared Error (MSE) and R-squared score.
####5) Visualization: Visualizing the best-fit line for key features and understanding the impact of each feature on car prices.

By the end of this analysis, we aim to create a robust model capable of predicting car prices and provide insights into the key factors affecting car value in the market.
"""

# Import necessary libraries
import numpy as np # NumPy is a powerful tool for numerical computations in Python
import matplotlib.pyplot as plt #Data Visualization
import pandas as pd # Pandas is a powerful library for data manipulation and analysis.

df = pd.read_csv('CarPricesPrediction.csv') #The df = pd.read_csv line reads a CSV file into a DataFrame named df using the pandas library.

df.head()

# Checking for missing values
df.isnull().sum()

"""There are no missing values in the dataset

For Logistic Regression, we will create a binary classification problem (e.g., "Expensive" vs. "Affordable"). We'll define the threshold for the price.
"""

# Create a binary variable 'Expensive' based on a price threshold
threshold = df['Price'].median()
df['Expensive'] = (df['Price'] > threshold).astype(int)

"""####Feature Scaling
Gradient Descent works best when the features are scaled, so we will standardize the numerical features.
"""

from sklearn.preprocessing import StandardScaler

# Select numerical features
numerical_features = ['Year', 'Mileage']

# Standardize the features
scaler = StandardScaler()
df[numerical_features] = scaler.fit_transform(df[numerical_features])

"""###Drop the Index Column"""

data_cleaned = df.drop(columns=['Unnamed: 0'])

df.head()

data_cleaned.head()

"""####One-Hot Encoding: Use pd.get_dummies() to convert the categorical variables (Make, Model, Condition) into numeric form.

This will prepare the data for performing linear regression using gradient descent. You can then split the data into training and testing sets and implement the gradient descent algorithm. Let me know if you'd like further guidance on these steps
"""

data_encoded = pd.get_dummies(data_cleaned, columns=['Make', 'Model', 'Condition'], drop_first=True)

"""## Step 1: Split the Dataset into Features and Target"""

# Split the dataset into features (X) and target (y)
X = data_encoded.drop('Price', axis=1)  # Features (dropping target column)
y = data_encoded['Price']               # Target (Price)

"""####Step 2: Split the Data into Training and Testing Sets
* Split the dataset into training and testing sets (80/20 split is common).
"""

from sklearn.model_selection import train_test_split

# Split data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

"""###Step 3: Standardize the Features
* Standardize the features to ensure they are on the same scale for better gradient descent performance.
"""

from sklearn.preprocessing import StandardScaler

scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

"""###Step 4: Initialize Weights and Bias
* Initialize the weights (coefficients) and bias term.
"""

import numpy as np

# Initialize weights to zeros (or random values) and bias
n_features = X_train_scaled.shape[1]
weights = np.zeros(n_features)  # Initialize weights to zeros
bias = 0  # Initialize bias

"""###Step 5: Define the Hypothesis Function (Linear Model)
* The linear hypothesis function is simply a weighted sum of inputs plus bias.
"""

def predict(X, weights, bias):
    return np.dot(X, weights) + bias

"""###Step 6: Define the Cost Function (Mean Squared Error)
* The cost function evaluates how well the model is performing.
"""

def compute_cost(X, y, weights, bias):
    m = len(y)
    predictions = predict(X, weights, bias)
    cost = (1 / (2 * m)) * np.sum((predictions - y) ** 2)
    return cost

"""###Step 7: Implement Gradient Descent
* Use gradient descent to update weights and bias to minimize the cost function.
"""

def gradient_descent(X, y, weights, bias, learning_rate, epochs):
    m = len(y)
    cost_history = []

    for i in range(epochs):
        predictions = predict(X, weights, bias)

        # Calculate gradients
        dW = (1 / m) * np.dot(X.T, (predictions - y))  # Gradient for weights
        dB = (1 / m) * np.sum(predictions - y)         # Gradient for bias

        # Update weights and bias
        weights -= learning_rate * dW
        bias -= learning_rate * dB

        # Compute the cost for each iteration
        cost = compute_cost(X, y, weights, bias)
        cost_history.append(cost)

        # Optionally print the cost every 100 epochs
        if i % 100 == 0:
            print(f"Epoch {i}, Cost: {cost}")

    return weights, bias, cost_history

"""###Step 8: Train the Model
* Train the linear regression model using gradient descent.
"""

# Set learning rate and number of epochs
learning_rate = 0.01
epochs = 3000

# Train the model
weights, bias, cost_history = gradient_descent(X_train_scaled, y_train, weights, bias, learning_rate, epochs)

"""###Step 9: Evaluate the Model
* After training, evaluate the model on the test set.
"""

from sklearn.metrics import mean_squared_error, r2_score

# Make predictions on the test set
y_pred = predict(X_test_scaled, weights, bias)

# Calculate performance metrics
mse = mean_squared_error(y_test, y_pred)
r2 = r2_score(y_test, y_pred)

print(f"Mean Squared Error: {mse}")
print(f"R-squared Score: {r2}")

"""###Step 10: Visualize the Cost Function (Optional)
* We can plot the cost over epochs to visualize how the cost decreases as the model trains.
"""

import matplotlib.pyplot as plt

plt.plot(cost_history)
plt.xlabel('Epochs')
plt.ylabel('Cost')
plt.title('Cost Reduction Over Time')
plt.show()

